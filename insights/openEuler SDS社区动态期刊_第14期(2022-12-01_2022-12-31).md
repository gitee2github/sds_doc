# openEuler SDS社区动态（2022-12-1~2022-12-31）

本期主编：张峰/刘秦飞

## 本期关键信息

- openEuler社区SDS动态
- SPDK 2022 PRC线上会议
- 近期社区合入PR概要
- 近期Ceph Developer动态

## openEuler社区SDS动态

#### openEuler-SDS-SIG
- 完成在openEuler20.03上支持ceph的多版本编译出包：12.2.8/14.2.8/15.2.5/15.2.15/16.2.7（发布在openEuler的编译出包指导）
- 发布2个openEuler实习任务（已有学生承接）：
	- 对主流开源存储软件ceph开展压缩、加密的性能优化以降低CPU开销（可借助鲲鹏硬件加速器）
	- 在openEuler 22.03-LTS-SP1使能ceph14.2.15/ceph15.2.15的编译出包和部署

## SPDK 2022 PRC线上会议 （2022年12月21日-2022年12月22日）
SPDK 2022中国虚拟论坛是一个在线论坛，SPDK专业人员和所有正在研究或在产品中使用SPDK的工程师，将聚集在一起讨论SPDK的最新功能特性。

还有更多议题详情见[Ceph 2022 PRC](https://web.cvent.com/event/c612045a-1a92-4f13-946d-526e120cc991/summary?i=K6VzhcIrTkqqwHIK2J0nLg),会议材料待主办方发布。

具体议题如下：
* **SPDK项目现状**
  * SPDK社区在今年继续扩展项目的功能，同时持续提高性能和高效处理的能力。在这个分享中，Jim会主要分享下在过去一年中的大的功能，以及在明年计划的一些特性。
* **最佳SPDK实践：阿里云存储五年演进的经验分享**
  * 在过去的五年里，阿里云块存储团队在基于SPDK的存储栈从内核空间向用户空间的演进中做出了巨大的突破。通过SPDK，我们使存储栈更加高效、灵活和更具CPU亲和性。本主题将分享过去五年我们从构建基于SPDK的存储系统中所积累的经验。
* **SPDK基于VFIO-USER的新虚拟化方案介绍**
  * 大家对SPDK的VHOST-USER虚拟化方案都比较熟悉了，针对虚机场景，在去年SPDK增加了NVMe接口的支持，该方案基于新的VFIO-USER协议实现，基于该协议可实现任意PCI设备的支持。今年，SPDK基于VFIO-USER增加了VIRTIO-BLK和VIRTIO-SCSI设备的支持。这里首先我们会详细介绍VIRTIO PCI抽象层的实现以及基于该层之上对VIRTIO-BLK和VIRTIO-SCSI的模拟；因为VFIO-USER和VHOST-USER有很多类似的地方，接下来我们也会提供两者间的性能数据对比，线程模型以及实现细节的差别；最后我们会分享下接下来的开发计划和社区开发进展等。
* **VDUSE架设用户态存储服务到容器的桥梁**
  * VDUSE (vDPA用户态设备) 是Linux中的一种技术，建立在vDPA内核子系统上，以提供框架在用户空间中实现软件模拟的vDPA设备。它提供了将用户空间服务连接到裸金属容器的理想方案。本次演讲将介绍VDUSE在字节跳动内的创新使用，来为容器构建最佳的存储基础设施。并且基于VDUSE最近在Linux内核中的增强，SPDK如何采用VDUSE来突破过去的制约，提供最佳的存储服务给容器。
* **SPDK在浪潮超融合分布式存储（SmartONE）的应用实践**
  * 随着NVMe存储介质的普及，DPU/IPU智能网卡和NVMe-oF存储协议的快速成熟，浪潮超融合分布式存储（SmartONE）应对这些新型的应用场景时，在原有架构上无法发挥其全部能力。而SPDK以其灵活的集成方式和用户态高性能的NVMe实现、网络模型抽象，为SmartONE提供了相关解决方案。本材料主要以SmartONE分布式存储实现为案例分享如何使用SPDK在原有分布式存储方案基础上做异步化框架改造、TCP和RDMA网络适配及NVMe-oF功能实现等。
* **SPDK NVMe over TCP通过DSA进行硬件CRC32C计算加速的应用**
  * 随着新平台新硬件加速特性(比如DSA)，介绍在SPDK NVMe over TCP模块中，来加速通过CRC32C进行数据保护的应用。
* **SPDK高性能存储开发套件中可追溯的多功能模糊测试最佳实践**
  * 在本次演示中，我们想展示我们的存储性能开发工具包 (SPDK) 验证团队在非常有限资源的情况下，如何完全改变了使用 LLVM libfuzzer进行模糊测试的持续集成的管理和执行方式，并且对项目进度没有影响。 通过这种可追溯的多功能模糊测试最佳实践以及模糊测试与 LLVM libfuzzer 的持续集成，我们平衡了 SPDK 产品的质量和效率。 这种最佳实践很容易部署到我们当前的持续集成系统中。
* **Ceph RBD浅析及性能调优**
  * Ceph是一种非常流行的分布式开源存储系统。Librbd是Ceph为块存储提供的库。它实现了RBD接口。上层应用程序（如fio和SPDK）可以通过调用librbd和librados的接口来操作Ceph集群，包括创建和删除池/图像、读取/写入数据等。本次分享将详细介绍RBD的IO栈和线程模型，然后分析性能瓶颈，简析常见的优化方案。
* **SPDK RBD bdev的应用以及性能调优**
  * Ceph作为一个开源很久的分布式文件系统，至今依然被广泛使用。SPDK提供了RBD bdev模块，借助librbd和librados实现了对Ceph RBD的支持。
本次分享将聚焦在SPDK RBD bdev模块的应用场景，然后结合相关测试工具，简析常见的性能优化问题。
* **基于Ceph NVMeoF Gateway的性能学习**
  * 这个项目主要探索了Ceph NVMeoF Gateway的性能，寻找multi-core/multi-volume情况下的性能瓶颈，并理解Ceph contexts对性能的影响。当把我们的Gatways方法和其他非Gateway方式进行比较时，我们的目标是只损失5~10%的性能。
* **阿里云本地盘实例软硬一体架构演进与实践**
  * 随着存储介质从SATA HDD 到 NAND SSD 再到 PMem 的不断演进，底层存储设备的 IO 性能得到了大幅提升。另一方面随着数据规模的爆发式增长，云上用户对存储服务的性能、稳定和可靠性提出了越来越高的要求。阿里云块存储团队面向云场景，通过存储软硬件的协同设计和优化，不断提升云存储的服务体验。阿里云最新发布的新一代弹性本地盘实例采用了创新的CIPU架构，提供多租户、高IOPS、高带宽、低时延能力，满足云应用场景。
* **通过Intel MEV IPU 加速存储应用**
  * Intel目前可以向客户提供基于ASIC的MEV IPU产品，其可以对主机提供NVMe、virtio的PF/VF用以访问IPU本地下挂的存储资源或者通过100G（或更高速率）网络连接的远端的存储资源，屏蔽后端的实现差异和适配的需求，卸载相关的适配逻辑，从而加速存储应用业务。基于此可以更好的实现软件定义的存储特性。 本处主要介绍Intel MEV IPU的基本参数、特性及其主要使用场景。
* **利用IPU卸载容器镜像的相关操作**
  * 本议题介绍如何把IPU和云原生有机的结合起来。
* **SPDK结合IPU针对存储卸载的加速以及其他应用**
  * 主要介绍SPDK软件与IPU结合在存储卸载与加速方面的应用。
* **CSAL/WSR项目进展分享**
  * 主要介绍CSAL构架演进和存储介质支持计划以及存储性能数据包括阿里云D3C实例的应用性能，英特尔P5810/P5820/ZNS的最新数据
  

## 近期Ceph社区合入PR概要

### 1. 概览
- 本月ceph社区新增449个PR, 新合入406个PR。
- 本月合入PR最多的厂商是redhat, 共计合入327个。
- 本月提交commit最多的开发者是Zac Dover, 共计提交62个commits。
#### 补丁数量统计

| 补丁新增  | 补丁合入 | 补丁关闭 | 补丁更新 |
| --------- | -------- | -------- | -------- |
| **449** | **406** | **136** | **935** |
#### 活跃开发者提交commit排行
1. Zac Dover
2. Kefu Chai
3. Casey Bodley
4. Ilya Dryomov
5. colemitchell
RedHat作为主要贡献者，依然在各个模块均有较多投入, 国内厂商inspur（浪潮）主要投入rgw的bug修复；

> 近期社区重要补丁汇总: [传送门](https://github.com/ceph/ceph/pulse)

#### 近期Ceph社区版本重要合入PR说明

- OSD:
  - 新增修改pg的主osd的工具 :[#49178](https://github.com/ceph/ceph/pull/49178)
  - ceph perf dump中新增slow ops请求数量统计 :[#47596](https://github.com/ceph/ceph/pull/47596)    
  -  随机化osd bench过程写入的数据，让osd bench测试更加准确 :[#48138](https://github.com/ceph/ceph/pull/48138)             
- crimson：
  - dump each shard seastar metrics ：[#49325](https://github.com/ceph/ceph/pull/49325)
  - 修复denc device_spec_t, device_type_t and segment_type_t:[#49496](https://github.com/ceph/ceph/pull/49496)
  - correct the behavior of reserving space：[#48691](https://github.com/ceph/ceph/pull/48691)
- mds：
  - 在拆分或合并目录时，增加判断快照项统计，避免mds崩溃：[#48907](https://github.com/ceph/ceph/pull/48907)
- RGW:
  - 新增一个inline_data zone放置规则选项，可以控制对象的第一个chunk是否放在head_object中，可以权衡读写操作与删除操作之间的性能：[#48711](https://github.com/ceph/ceph/pull/48711)
  - 遍历顺序删除调整为并发删除，提升删除性能：[#48679](https://github.com/ceph/ceph/pull/48679)         
- bluestore
  - 完善 cache_onode 、cache_buffer内存池统计方法: [#40066](<https://github.com/ceph/ceph/pull/40066>)

## 近期Ceph Developer动态

#### 会议汇总

社区核心开发者会议如下（由于与美国东部时间有13个小时的时差，以事后查阅社区录像为主）。[传送门](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw)

| 序号 | 会议名称                             | 说明                    | 频率 |
| ---- | ----------------------------------- | ----------------------- | ---- |
| 1    | Crimson SeaStore OSD Weekly Meeting | 下一代版本seastore开发   | 周   |
| 2    | Ceph Orchestration Meeting          | Ceph管理模块（Mgr）开发  | 周   |
| 3    | Ceph DocUBetter Meeting             | 文档优化                | 双周 |
| 4    | Ceph Performance Meeting            | Ceph性能优化            | 双周 |
| 5    | Ceph Developer Monthly              | Ceph开发者月度例会       | 月   |
| 6    | Ceph Testing Meeting                | 版本验证及发布例会       | 月   |
| 7    | Ceph Science User Group Meeting     | Ceph用于科学计算领域     | 月   |

#### 会议小结

- Crimson SeaStore OSD Weekly Meeting
  - 当前正在实现messager、seastore的multicore功能；
  - 360计划在seastore中增加data cache功能，社区想明确收益场景和提升比例，相关数据需进一步补充，继续保持关注。

---
【如关注分布式存储技术请加小编好友微信“liu_qinfei”，拉你进sig-SDS技术交流群】

> 上一期：[openEuler SDS社区动态（2022-11-1~2022-11-30）](https://zhuanlan.zhihu.com/p/589162784)
