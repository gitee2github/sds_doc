# openEuler SDS社区动态（2022-11-1~2022-11-30）

本期主编：陈续强/刘秦飞

## 本期关键信息

- openEuler社区SDS动态
- Ceph Virtual 2022
- 近期社区合入PR概要
- 近期Ceph Developer动态

## openEuler社区SDS动态

#### openEuler-SDS-SIG
- 开展分布式存储组件的性能优化、ARM使能实践等工作，发布了如下多篇实践指南。
  * [鲲鹏硬件加速器ceph使能指南](https://gitee.com/src-openeuler/ceph/wikis/%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/%E9%B2%B2%E9%B9%8F%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E5%99%A8ceph%E4%BD%BF%E8%83%BD%E6%8C%87%E5%8D%97)
  * [openEuler DAOS出包指导文档](https://gitee.com/src-openeuler/ceph/wikis/%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/openEuler%20DAOS%E5%87%BA%E5%8C%85%E6%8C%87%E5%AF%BC%E6%96%87%E6%A1%A3%20%E5%86%AF%E6%98%A5%E6%9D%BE(%E5%8D%8E%E4%B8%BA))
  * [Ceph编译速度提升总结](https://gitee.com/src-openeuler/ceph/wikis/%E6%8A%80%E6%9C%AF%E6%96%87%E6%A1%A3/Ceph%E7%BC%96%E8%AF%91%E9%80%9F%E5%BA%A6%E6%8F%90%E5%8D%87%E6%80%BB%E7%BB%93%20%E9%99%88%E7%BB%AD%E5%BC%BA%40chenxuqiang2022)
  * [Lustre arm64初探](https://zhuanlan.zhihu.com/p/586170722)
- 新特性
  * [uadk压缩插件，支持块与对象压缩处理加速](https://gitee.com/openeuler/ceph_dev/pulls/6)
  * [rgw SSE-C和SSE-KMS支持SM4国密算法](https://gitee.com/openeuler/ceph_dev/pulls/10)
  * [消息线程间负载均衡优化，实现性能提升，4K随机写性能提升9%](https://gitee.com/openeuler/ceph_dev/pulls/9)
- 社区动态
  * ceph作为一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式文件系统软件，已在申威3231服务器平台上支持使用。目前申威已经实现ceph v16.2在openeuler22.03 LTS操作系统上的移植和适配，满足各项基础功能的运行。ceph为申威提供文件系统、块存储和对象存储，分布式动态扩展，在后续开发中可作为openstack 的后端存储来提高数据转发效率。

#### [openEuler Meetup-分布式存储与大数据](https://mp.weixin.qq.com/s/V1wbtaWmusOLnIICHOhMGg)
2022年11月10日，openEuler社区联合openEuler SDS SIG和bigdata SIG开展的“openEuler Meetup—分布式存储与大数据”圆满结束。活动吸引了来自ISV、互联网、运营商等30+企业近两百名专家和开发者踊跃参与。线上会议、交流群讨论热烈，开发者在提问环节对分布式存储及大数据领域关键技术与讲师进行深入的交流互动。议题包括：
* **一种Spark SQL Native引擎实现SQL加速** 陈强（华为）
一种Spark SQL Native引擎解决方案，通过执行层可灵活定制算子实现策略，兼容codegen和静态编译方式，基于列式内存布局实现向量化，结合LLVM动态优化生成代码，提升Spark SQL的性能。
* **使用bcache加速Ceph向openstack提供高性能存储** 杨晓亮（统信）
由于openstack本地存储已无法满足数据安全性要求。ceph凭借其高扩展、高可靠、高性能等显著特性，成为openstack持久化存储的首选。然而由于网络带宽，磁盘性能及硬件设施成本等因素，千兆网卡+HDD的ceph组合已经难以满足openstack高并发操作的需求。基于此，可优化传统的ceph存储组合，使用SSD+HDD+万兆网卡的组合形式，使用bcache缓存方案加速OSD，向openstack提供高性能持久化存储方案。
* **元数据缓存策略对Ceph性能的影响** 池信泽（XSKY）
介绍Ceph的总体架构以及元数据缓存策略对性能影响。
* **Apache Hadoop发行版的研发探索** 童小军（红象云腾）
介绍Redoop Enterprise发行版本研发和演化路径，对比其他发行版本分析背后策略，探索商业发行版本技术策略、驱动力和发展模式。
* **利用五步法 & A-Tune优化ceph性能** 孙立刚（麒麟软件）
结合具体项目案例介绍利用五步法优化分布式存储软件ceph，以及感知上层业务场景、自动完成调优配置的智能调优工具A-Tune
* **Mayastor & SPDK on Arm64 openEuler** 刘新良（Linaro）
SPDK作为一个高性能的用户态存储基础开发库套件天然集成NVMe-oF RDMA 和 NVMe-oF TCP，极大地方便存储数据面的开发工作。当前多数云原生存储开源项目和传统的存储项目都是基于SPDK支持了NVMe-oF功能。本次分享将讲解SPDK和Mayastor在ARM64 openEuler上的软件生态使能情况，二者在ARM64 openEuler上的性能测试结果，以及在过程中遇到的问题和性能测试经验等。
* **存储性能优化六板斧** 冯春松（华为）
结合项目实践经验，分享通过六种优化手段（诸如调度、部署、瓶颈点、微架构优化，网卡中断绑核，基础性能库），提升分布式存储性能。
* **Apache Spark 官方容器镜像的那些事儿** 姜逸坤（Apache Spark Committer, openEruler Infra Maintainer， 华为）
Apache Spark是一款分布式内存计算的统一分析引擎；而随着云原生架构的逐渐成熟，Spark容器化部署也越来越成熟，一个满足用户需求、高质量的应用容器镜像变得尤为重要。本次分享介绍了Apache Spark 官方容器镜像支持的那些事儿，包括：Apache Spark社区容器化项目规划和技术路线，在不同架构和操作系统的支持和实践。
## Ceph Virtual 2022

Ceph Virtual在2022年11月3日~11月16日期间举行，对Ceph RGW，Crimson，Ceph网络， 性能，容器化，安全等方面均有探讨，冯春松作为Ceph Messenger Maintainer发表议题演讲《Optimize Ceph messenger Performance》，值得关注。还有更多性能优化详情见[Ceph-virtual](https://ceph.io/en/community/events/2022/ceph-virtual/), 回放视频: [直播回放](https://www.youtube.com/watch?v=KJkyV-fJvow&list=PLrBUGiINAakOw2jF3Bz0L01tUKNAwnPA0&ab_channel=Ceph)

具体议题如下：
* **Ceph Crash Telemetry - Observability in Action**
  * Ceph Crash Telemetry是一个允许用户自动报告匿名故障的模块，通过后端运行的一些工具，检测集群Crash报告的相似性，然后将其提交到Ceph的bug跟踪系统，以帮助开发人员/社区检测生产系统中遇到的新出现的和频繁出错的问题。该演讲主要介绍了Ceph Crash Telemetry的原理以及用户如何从该模块中收益以及如何做出贡献；
* **Operating Ceph from the Ceph Dashboard: past, present and future**
  * 该演讲介绍了Ceph Dashboard的过去，现在和将来；
* **Putting the Compute in your Storage**
  * 该演讲介绍了基于librados进行编程的基础，包括键值存储，原子事务，对象克隆和快照支持。并介绍了如何利用Ceph OSD提供的可以执行任意代码的能力来扩展对象接口，将计算下推到存储节点的方法。
* **Optimizing RGW Object Storage Mixed Media through Storage Classes and Lua Scripting**
  * S3  Storage Class是一种将数据引导到满足特定弹性、成本和性能要求的底层介质上的方法。例如，可以为 SSD 或 HDD 介质、副本与纠删码池等定义 RGW 后端Storage Class，该演讲介绍了通过Stroage Class和Lua脚本对RGW对象混合存储介质中的TLC和QLC介质，通过Stroage Class功能将大对象分配到QLC SSD介质中，并减少存储空间浪费；
* **How we operate Ceph at scale**
  * 该演讲讨论了在快速增长的 块存储和对象存储工作负载环境中，运营不同规模集群的策略和挑战。
* **Ceph and 6G: Are we ready for zettabytes?**
  * Ceph是能够满足5G存储需求和边缘数据中心的存储解决方案，但是面对未来的6G网络，Ceph该如何应对这些挑战？
* **What's new with Crimson and Seastore?**
  * Crimson是针对下一代存储设备的解决方案，旨在最大限度地减少CPU开销，并提高IO吞吐和延迟。而Seastore是Crimson-osd新的后备存储。该演讲针对新型存储技术，包括持久内存和ZNS设备，介绍Crimson和Seastore的最新进展。
* **Understanding SeaStore through profiling**
  * Seastore是新的ObjectStore，旨在支持新一代的存储接口和技术（NVMe，ZNS，持久内存等）。该演讲介绍了SeaStore的内部工作原理，以及如何进行性能分析，并基于这些分析进行性能优化的方法，同时介绍了当前SeaStore的开发状态以及与BlueStore的差距。
* **Accelerating PMEM Device operations in bluestore with hardware based memory offloading technique**
  * 数据中心配备越来越多的快速设备，如持久内存，但是持久内存没有提供与DMA相关的功能，因此需要繁重的工作负载来驱动这些设备，这就导致了高昂的CPU开销。该演讲介绍了如何使用内存卸载设备（如DSA）减轻IO过程中的CPU压力。
* **S3select: Computational Storage in S3**
  * S3select实现了S3的下推范式，仅从对象中提取需要的数据，这可以显著提高性能，并降低应用程序需要访问S3中的数据的网络带宽。该演讲介绍了s3slect的操作和架构。描述了什么是下推技术，以及该技术在何种应用场景中会获得收益。
* **Optimize Ceph messenger Performance**
  * 该演讲介绍了针对Ceph的Messenger模块的性能优化技术。通过使能网卡的SR-IOV功能，使每个OSD都使用专用的VF网卡；新增了DPDK中断模式；实现了单CPU核心和多网卡队列；新增管理套接字命令来获取网卡状态，采集统计信息，定位故障；通过调整CEPH的限流参数，TCP/DPDK数据包发送和接受缓冲区的大小，从而防止数据丢失和重传；通过以上技术大大提升了Ceph Messenger的性能。
* **RGW Zipper**
  * RGW Zipper将RGW分为了上下两层，上层是包含了S3/Swift的操作（Ops）,下层则称为Store,包含了如何存储数据和元数据的详细信息。这样的分层解耦结构为下层的Store提供了更多的灵活性。该演讲介绍了为RGW Zipper Store开发的新的名为DBStore的存储层，它将数据存储在SQL中，特别是本地SQLite数据库中。同时提供了一些中间筛选器层的转换操作和执行策略，如LuaFilter。
* **Revealing BlueStore Corruption Bugs in Containerized Ceph Clusters**
  * 在容器化的Ceph集群中，存在大量的Bluestore损坏的问题，大多数问题发生在OSD创建和重启过程中。由于容器化的集成测试用户发生大量的OSD创建（这些问题在传统的Ceph集群中并不常见），因此如何克服这类问题，是该演讲的核心内容。以及对QA流程的改进建议，以防止将来再次出现类似的问题。
* **Data Security and Storage Hardening In Rook and Ceph**
  * 该演讲介绍了Ceph和Rook中的数据安全问题，对端到端的数据安全问题做了全面的介绍，包括威胁模型，安全区域等概念，介绍了静态和动态加密，用户访问控制，密钥管理，数据保留和安全删除等问题。而Rook使得安全强化选项的过程更加容易。
* **DisTRaC: Accelerating High-Performance Compute Processing for Temporary Data Storage**
  * 科学界和学术界越来越多的使用对象存储在HPC集群中存储和处理器数据，但是对象存储不一定是为高性能计算设计的，而是更加适合长期存储。这也是为什么当前大量的HPC应用都使用高性能文件系统来处理数据。但是网络文件系统可能存在一个用户破坏网络会影响集群中的所有其他人数据处理的性能。因此该演讲介绍了一个解决方案：DisTRaC：(Dis)tributed (T)raisent (Ra)m (C)eph。DisTRaC提供了一种使用RAM将Ceph部署到其HPC集群中的方法。减少了网络文件系统的IO开销，并提供了潜在的数据处理性能提升；
* **New workload balancer in Ceph**
  * Quincy版本的新功能之一，又称为primary balancer；之前的upmap balancer在容量均衡方面工作良好，但是不能均衡所有OSD上的负载，这是一个性能问题，尤其是在小型集群和PG较少的池中。在该演讲中讨论了容量均衡和负载均衡之间的区别，以及primary balancer未来的规划；
* **NVMe-over-Fabrics support for Ceph**
  * NVMe-over-Fabrics是高性能远程块存储访问中开放的，且广泛使用的标准，越来越多的存储供应商正在引入NVMeoF，为NVMEoF的target和initiators提供硬件卸载。Ceph不支持用户块存储访问的NVMEoF协议。如何结合Ceph RADOS协议和NVMeoF协议的优点是目前研究的热点。该演讲介绍了Ceph RBD集成原生的NVMeoF的实现和遇到的挑战，包括子系统/命名空间发现，容错，性能，身份验证和访问控制等。并介绍了如何扩展NVMeoF target的设计，便于通过Ceph CRUSH算法来减少额外的网络跃点。
* **Introduction to Container Object Storage Interface aka COSI for ceph RGW**
  * 该K8S应用程序，CSI提供了一种为其工作负载使用文件/块存储的方法，COSI的目的就是为对象存储提供类似的功能，该演讲介绍了COSI的实现原理，以及没有为对象定义标准协议的挑战。
* **Troubleshooting and Debugging in the rook-ceph cluster**
  * 该演讲介绍了如何对rook-ceph集群进行故障排除，讨论了一些故障排除方案和rook项目未来的路线图。

## 近期Ceph社区合入PR概要

RedHat作为主要贡献者，依然在各个模块均有较多投入, Intel在Crimson方面有较多的投入；

> 近期社区重要补丁汇总: [传送门](https://github.com/ceph/ceph/pulse)

#### 近期Ceph社区版本重要合入PR说明

- BlueStore:
  - 解决Bluestore的`bluestore_prefer_deferred_size`的bug，当IO请求小于`bluestore_prefer_deferred_size`时将启用延迟写入技术，反之使用立即写入磁盘设备:[#48490](https://github.com/ceph/ceph/pull/48490)
- crimson：
  - crimson osd: 增加Pool EIO flag：[#49029](https://github.com/ceph/ceph/pull/49029)
  - 修复各种和fmt相关的bug:[#49011](https://github.com/ceph/ceph/pull/49011)
  - 重新实现了`crimson::do_for_each`函数，避免当`seastar::need_preempt == true`时导致的栈溢出问题：[#48932](https://github.com/ceph/ceph/pull/48932)
  - seastore: 通过在replaly期间添加deallocation-Map，过滤掉过期的增量数据: [#48773](https://github.com/ceph/ceph/pull/48773)
  - 修复了alienstore线程的cpu_set格式问题，支持了列表格式, 如"1-3,5-6,7-9":[#48716](https://github.com/ceph/ceph/pull/48716)
  - 修复file_stat中无法正确获取块设备大小的bug：[#48693](https://github.com/ceph/ceph/pull/48693)
  - 简化clone操作：[#48527](https://github.com/ceph/ceph/pull/48527)
- mon：
  - 修复GCC13下编译告警：[#48661](https://github.com/ceph/ceph/pull/48661)
- msg:
  - 为Windows客户端增加了一个基于poll()函数的事件驱动程序，解决基于select()函数的驱动程序遇到的文件描述符限制问题：[#46525](https://github.com/ceph/ceph/pull/46525)
- RBD
  - rbd/mgr: 修复在Pool删除时，避免向任务队列中插入任务的bug: [#49036](<https://github.com/ceph/ceph/pull/49036>)
  - librbd/cache: 修复pwl clean和bytes_dirty cache状态不一致的问题: [#48542](https://github.com/ceph/ceph/pull/48542)
- RGW
  - notification: 修复多段上传过程中bug：[#48877](https://github.com/ceph/ceph/pull/48877),[#48875](https://github.com/ceph/ceph/pull/48875) 
## 近期Ceph Developer动态

#### 会议汇总

社区核心开发者会议如下（由于与美国东部时间有13个小时的时差，以事后查阅社区录像为主）。[传送门](https://www.youtube.com/channel/UCno-Fry25FJ7B4RycCxOtfw)

| 序号 | 会议名称                             | 说明                    | 频率 |
| ---- | ----------------------------------- | ----------------------- | ---- |
| 1    | Crimson SeaStore OSD Weekly Meeting | 下一代版本seastore开发   | 周   |
| 2    | Ceph Orchestration Meeting          | Ceph管理模块（Mgr）开发  | 周   |
| 3    | Ceph DocUBetter Meeting             | 文档优化                | 双周 |
| 4    | Ceph Performance Meeting            | Ceph性能优化            | 双周 |
| 5    | Ceph Developer Monthly              | Ceph开发者月度例会       | 月   |
| 6    | Ceph Testing Meeting                | 版本验证及发布例会       | 月   |
| 7    | Ceph Science User Group Meeting     | Ceph用于科学计算领域     | 月   |

#### 会议小结

- Ceph Performance Meeting
  - 2022-12-1
    - 使用 dm-crypt (cryptsetup) 的`no_read_workqueue` 和 `no_write_workqueue`选项提高性能，在AMD EPYC7320 16 Core performance CPU模式下，fio的4K写[性能对比](https://lists.ceph.io/hyperkitty/list/ceph-users@ceph.io/thread/D5URLVPVGX52O6WYV474T6IVTHGHCASB/)：
      * 禁用`no_read/write_workqueue`时：IOPS=34.8K；
      * 启用`no_read/write_workequeue`时：IOPS=55.7K，提升了60%以上；
    - Pacific的磁盘写存在写放大，延迟高；
    - 100亿个小文件的MDS性能讨论;
    - 对硬RAID的性能调优实践：包括read/write cache和stripe_size;
  - 2022-11-17
    - 对BlueStore中的共享Blob进行了讨论，共享Blob在快照场景下有较好的性能表现，但是还有一些问题：如磁盘空间写放大，和CPU使用率高的问题。目前还在进一步的定位和讨论中；
    - 继续讨论了BlueStore上面的范围删除问题，对于范围删除可能带来的各种问题，会议依然持有很多的怀疑状态，应该还会继续讨论下去；

- Crimson SeaStore OSD Weekly Meeting
  - Crimson计划在Reef版本推出tech preview版本；
  - 当前以稳定性优先，开发MultiCore、snap，scrub等关键功能，性能优化需要在功能稳定后投入；
  - 随着intel optane pemem项目撤销，Crimson后续也不会再考虑pmem设备；
  - OSD MultiCore：一个OSD使用一个设备，可以有效的管理设备故障；而将盘划分为多个分区，对应多个OSD的方式在盘故障的模式下，难以管理。
---
【如关注分布式存储技术请加小编好友微信“liu_qinfei”，拉你进sig-SDS技术交流群】

> 上一期：[openEuler SDS社区动态（2022-10-1~2022-10-31）](https://zhuanlan.zhihu.com/p/581332698)

